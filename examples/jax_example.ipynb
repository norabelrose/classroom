{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting a reward model offline**\n",
    "\n",
    "In this example, we'll train a vanilla PPO agent on the Brax Ant environment using the \"ground truth\" reward provided by the environment. We'll then fit a reward model to the resulting clips.\n",
    "\n",
    "In order to do this, we'll need to \"record\" a subset of the environment states visited by the agent during the training process. For this we can use the `BraxRecorder` environment wrapper class, which uses JAX host callbacks to efficiently record these states without interrupting Brax's wicked fast all-GPU training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nora/miniconda3/lib/python3.10/site-packages/jax/_src/tree_util.py:188: FutureWarning: jax.tree_util.tree_multimap() is deprecated. Please use jax.tree_util.tree_map() instead as a drop-in replacement.\n",
      "  warnings.warn('jax.tree_util.tree_multimap() is deprecated. Please use jax.tree_util.tree_map() '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<function brax.training.ppo.make_inference_fn.<locals>.inference_fn(params, obs, key)>,\n",
       " ((DeviceArray(29491200., dtype=float32),\n",
       "   DeviceArray([ 5.76340258e-01,  9.01916504e-01,  2.58659804e-03,\n",
       "                -4.69714850e-02, -1.90610871e-01,  9.07656997e-02,\n",
       "                 7.35080302e-01, -3.31008099e-02, -7.79484153e-01,\n",
       "                 6.62133619e-02, -6.69017971e-01, -3.86176594e-02,\n",
       "                 7.17971802e-01,  4.26348972e+00, -7.26184100e-02,\n",
       "                 2.89751794e-02, -1.77051239e-02,  1.05141617e-01,\n",
       "                -4.74078115e-03,  2.32848823e-02, -6.55472726e-02,\n",
       "                 3.36886235e-02,  7.00239167e-02,  2.19721422e-02,\n",
       "                 8.27182457e-02, -5.65480115e-03, -1.02745250e-01,\n",
       "                 2.02341871e-05, -4.19983735e-06, -1.41219217e-02,\n",
       "                 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "                -2.98183281e-02, -1.10408422e-02, -1.39004156e-01,\n",
       "                 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "                -1.14823297e-01, -4.22480404e-02, -3.19424361e-01,\n",
       "                 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "                -2.05941424e-01, -5.51866554e-02, -2.75339425e-01,\n",
       "                 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "                -2.61919647e-02,  8.61841962e-02, -2.30259448e-01,\n",
       "                 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "                -1.08227805e-05, -3.66421664e-05, -2.15438134e-09,\n",
       "                 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "                -6.19927160e-02,  6.61713108e-02,  1.61615113e-04,\n",
       "                 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "                -1.68372899e-01,  1.35930292e-02,  1.13771083e-02,\n",
       "                 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "                 4.85606901e-02,  8.05615168e-03, -1.93679258e-02,\n",
       "                 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "                 1.61893129e-01,  2.71712616e-02, -1.22948363e-03,\n",
       "                 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],            dtype=float32),\n",
       "   DeviceArray([3.0700984e+05, 1.5348981e+06, 3.8595041e+05, 4.8204319e+05,\n",
       "                1.9618545e+06, 4.4366830e+06, 2.0261426e+06, 4.2981155e+06,\n",
       "                1.5356448e+06, 4.3843650e+06, 1.5134025e+06, 4.7393715e+06,\n",
       "                1.4536045e+06, 1.5435306e+08, 1.4185915e+07, 2.1006698e+07,\n",
       "                5.4051848e+07, 3.9254428e+07, 1.4570856e+08, 5.5567354e+08,\n",
       "                2.9165843e+08, 6.3824646e+08, 2.5535026e+08, 6.7330330e+08,\n",
       "                2.0452869e+08, 5.9665210e+08, 2.2934250e+08, 6.4791768e+03,\n",
       "                6.5846465e+03, 3.2059628e+05, 1.0000000e+00, 1.0000000e+00,\n",
       "                1.0000000e+00, 1.4669320e+06, 1.3061886e+06, 3.7451360e+06,\n",
       "                1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 4.1422192e+06,\n",
       "                5.3819895e+06, 6.7729110e+06, 1.0000000e+00, 1.0000000e+00,\n",
       "                1.0000000e+00, 5.2679880e+06, 4.4319275e+06, 6.1383035e+06,\n",
       "                1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 2.0768799e+06,\n",
       "                3.0977035e+06, 5.6806410e+06, 1.0000000e+00, 1.0000000e+00,\n",
       "                1.0000000e+00, 2.5587016e+04, 2.5257131e+04, 1.0024037e+00,\n",
       "                1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.8601991e+06,\n",
       "                2.0128134e+06, 7.8315305e+04, 1.0000000e+00, 1.0000000e+00,\n",
       "                1.0000000e+00, 4.3909650e+06, 3.4709112e+06, 4.8347153e+05,\n",
       "                1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 2.2670265e+06,\n",
       "                1.8807010e+06, 8.4738500e+05, 1.0000000e+00, 1.0000000e+00,\n",
       "                1.0000000e+00, 4.1336470e+06, 2.8903678e+06, 1.2764702e+05,\n",
       "                1.0000000e+00, 1.0000000e+00, 1.0000000e+00], dtype=float32)),\n",
       "  FrozenDict({\n",
       "      params: {\n",
       "          hidden_0: {\n",
       "              bias: DeviceArray([ 6.5888815e-02,  3.1335175e-01,  4.0197453e-01,\n",
       "                            6.0059275e-02,  1.3026579e-01,  1.5963258e-01,\n",
       "                            1.9374996e-01, -9.3317777e-04,  2.5858253e-01,\n",
       "                            2.6516002e-01,  1.6222419e-01,  1.2645964e-01,\n",
       "                            1.9639744e-01,  1.2960672e-01,  2.4604790e-01,\n",
       "                            2.9702473e-01,  1.5195723e-01, -1.5985444e-01,\n",
       "                            1.7639861e-01,  7.1728863e-02, -6.4701468e-02,\n",
       "                            9.4478488e-02,  7.0254495e-03,  1.0433426e-01,\n",
       "                            1.7786199e-01,  2.2160339e-01,  1.9661236e-01,\n",
       "                            1.9511697e-01,  5.2115317e-02, -2.7264489e-04,\n",
       "                           -1.0558513e-01,  1.2030195e-01], dtype=float32),\n",
       "              kernel: DeviceArray([[-0.14804101,  0.10044017,  0.41138414, ..., -0.14113414,\n",
       "                            -0.6044065 ,  0.09303002],\n",
       "                           [-0.20571733,  0.3795685 ,  0.55411804, ..., -0.01782737,\n",
       "                             0.43247616,  0.18715376],\n",
       "                           [-0.26213282,  0.48734298,  0.44497964, ...,  0.04116933,\n",
       "                             0.7683002 ,  0.283647  ],\n",
       "                           ...,\n",
       "                           [-0.16600642,  0.00433656,  0.14464897, ...,  0.13421431,\n",
       "                            -0.04865967, -0.01344272],\n",
       "                           [ 0.15216687, -0.07775391, -0.0541596 , ...,  0.06580152,\n",
       "                            -0.15130655, -0.13748644],\n",
       "                           [ 0.11925934,  0.0037669 , -0.15873145, ..., -0.1053205 ,\n",
       "                             0.05936272, -0.1775032 ]], dtype=float32),\n",
       "          },\n",
       "          hidden_1: {\n",
       "              bias: DeviceArray([ 0.22258648,  0.22905277,  0.06532644,  0.07958634,\n",
       "                            0.1127964 , -0.12386877, -0.2552235 ,  0.1260066 ,\n",
       "                            0.18290275, -0.10010841,  0.01876432,  0.0245661 ,\n",
       "                            0.32509527,  0.09962444,  0.13541707, -0.17419323,\n",
       "                            0.03951512,  0.09794313, -0.02215649, -0.03631856,\n",
       "                           -0.19910765,  0.09086366,  0.07948631,  0.22080593,\n",
       "                            0.07796074,  0.10676013,  0.06998692,  0.0511652 ,\n",
       "                            0.07208478,  0.10156646, -0.12389192,  0.23887002],            dtype=float32),\n",
       "              kernel: DeviceArray([[-0.05471999,  0.11007572,  0.34405857, ...,  0.10481793,\n",
       "                            -0.2243854 , -0.02404012],\n",
       "                           [ 0.23172252,  0.3289386 , -0.07936315, ..., -0.11409646,\n",
       "                             0.15132357, -0.02247801],\n",
       "                           [-0.02310215,  0.17188284, -0.0658683 , ..., -0.4008587 ,\n",
       "                             0.24868493,  0.10571978],\n",
       "                           ...,\n",
       "                           [-0.2432109 , -0.6417685 , -0.30085942, ..., -0.24494281,\n",
       "                            -0.115253  ,  0.06144873],\n",
       "                           [ 0.2532281 , -0.21675731, -0.0862665 , ...,  0.0829101 ,\n",
       "                             0.04324164, -0.3676727 ],\n",
       "                           [-0.00841393, -0.19305824, -0.10958311, ..., -0.09140832,\n",
       "                            -0.05989468, -0.08653401]], dtype=float32),\n",
       "          },\n",
       "          hidden_2: {\n",
       "              bias: DeviceArray([-0.08293314,  0.04785217,  0.17528826,  0.16087675,\n",
       "                           -0.05747229,  0.23689066,  0.40398479, -0.0661807 ,\n",
       "                            0.20424275, -0.03527486,  0.00219024,  0.23641193,\n",
       "                            0.20010969,  0.21196264, -0.0243996 ,  0.19333236,\n",
       "                            0.18939564,  0.00734968, -0.00516851,  0.1758087 ,\n",
       "                           -0.0068387 ,  0.03286643,  0.05593793,  0.08416798,\n",
       "                            0.02999629, -0.06269717, -0.19479935, -0.05289051,\n",
       "                            0.37591624, -0.12022893, -0.21087945, -0.05460462],            dtype=float32),\n",
       "              kernel: DeviceArray([[-1.07349232e-01, -2.40616351e-01, -2.24972561e-01, ...,\n",
       "                             1.03705436e-01,  2.73970574e-01,  1.77260622e-01],\n",
       "                           [-4.35975075e-01, -1.48650706e-01, -1.40102729e-01, ...,\n",
       "                            -6.96851732e-03, -2.58265138e-01,  1.23972245e-01],\n",
       "                           [-2.85440147e-01, -2.05449890e-02, -3.26791704e-01, ...,\n",
       "                            -3.75103112e-03, -1.12574615e-01,  3.14609170e-01],\n",
       "                           ...,\n",
       "                           [-3.13392639e-01, -4.24739778e-01,  2.81670868e-01, ...,\n",
       "                            -1.19955823e-01, -8.71674914e-04,  3.87721717e-01],\n",
       "                           [-2.85858959e-01, -7.28603676e-02, -7.89952725e-02, ...,\n",
       "                            -3.13233614e-01,  2.35855669e-01, -2.15597838e-01],\n",
       "                           [-8.74305069e-01, -1.35633335e-01, -1.31500617e-01, ...,\n",
       "                            -3.56741309e-01, -3.22432637e-01,  1.67138442e-01]],            dtype=float32),\n",
       "          },\n",
       "          hidden_3: {\n",
       "              bias: DeviceArray([ 0.08133472,  0.09042867, -0.09364899,  0.09022991,\n",
       "                            0.19662438,  0.11816997, -0.02361625,  0.14692466,\n",
       "                            0.08404493, -0.0041927 , -0.25869823,  0.2178968 ,\n",
       "                           -0.14637858,  0.07617866,  0.13559404, -0.09120893,\n",
       "                           -0.30483085,  0.07686321,  0.33382028,  0.17275442,\n",
       "                           -0.07107215,  0.21612236, -0.132393  ,  0.00162055,\n",
       "                            0.15229797,  0.22532132, -0.10492495, -0.00750354,\n",
       "                           -0.27921793,  0.1237263 ,  0.14613642, -0.1342702 ],            dtype=float32),\n",
       "              kernel: DeviceArray([[-0.1182541 , -0.12998626,  0.24366626, ..., -0.18856996,\n",
       "                            -0.10831537,  0.24642   ],\n",
       "                           [ 0.06255289,  0.3020913 ,  0.35510185, ...,  0.18923229,\n",
       "                             0.05776257, -0.2947531 ],\n",
       "                           [ 0.15712543, -0.09069283, -0.02596411, ...,  0.41001147,\n",
       "                            -0.08862103, -0.13652559],\n",
       "                           ...,\n",
       "                           [-0.04369009, -0.17082705,  0.28896058, ...,  0.28253773,\n",
       "                            -0.4761158 ,  0.25611088],\n",
       "                           [-0.32198352,  0.10674793,  0.37145847, ..., -0.05062525,\n",
       "                            -0.48654947,  0.3350168 ],\n",
       "                           [-0.3644877 , -0.17590775, -0.4296177 , ...,  0.03593289,\n",
       "                            -0.18269955,  0.5306883 ]], dtype=float32),\n",
       "          },\n",
       "          hidden_4: {\n",
       "              bias: DeviceArray([-0.02452493, -0.024684  , -0.00238056, -0.01522164,\n",
       "                            0.04942049, -0.00680209, -0.02717886,  0.00508591,\n",
       "                           -0.34998628, -0.29633334, -0.3865663 , -0.31277275,\n",
       "                           -0.2619241 , -0.19171312, -0.31936318, -0.3238425 ],            dtype=float32),\n",
       "              kernel: DeviceArray([[-6.04738332e-02,  1.89468339e-01,  2.69202977e-01,\n",
       "                             9.70199257e-02, -2.21231610e-01,  4.11106110e-01,\n",
       "                             5.55983007e-01,  5.00184409e-02, -1.67446822e-01,\n",
       "                            -3.64822030e-01, -1.66135415e-01, -4.95526232e-02,\n",
       "                            -2.64744490e-01, -2.62288451e-01, -9.98895839e-02,\n",
       "                             2.32375681e-01],\n",
       "                           [ 2.03024998e-01, -3.52642417e-01, -1.72408819e-01,\n",
       "                             2.78926432e-01,  1.06623277e-01,  4.04339619e-02,\n",
       "                             9.95199829e-02,  1.57066837e-01, -3.80663633e-01,\n",
       "                            -1.98886678e-01,  3.64839248e-02,  9.12766904e-02,\n",
       "                            -2.05823183e-02, -1.09881580e-01,  3.00271779e-01,\n",
       "                            -1.89725369e-01],\n",
       "                           [-2.33571425e-01,  3.55331868e-01, -3.57031345e-01,\n",
       "                            -2.17779189e-01, -2.13689521e-01,  2.91752219e-01,\n",
       "                            -1.29247472e-01,  8.01728144e-02,  1.78734303e-01,\n",
       "                             4.93477941e-01,  7.27767169e-01,  2.43038461e-01,\n",
       "                             6.17393367e-02,  3.24169755e-01,  1.48025975e-01,\n",
       "                             4.43763971e-01],\n",
       "                           [ 5.15342832e-01,  9.04352888e-02, -2.02618256e-01,\n",
       "                            -2.87297387e-02,  3.93666238e-01,  4.31720428e-02,\n",
       "                             2.42289633e-01,  2.72240818e-01,  4.29159462e-01,\n",
       "                            -5.65941595e-02, -2.25450069e-01, -1.75873294e-01,\n",
       "                            -8.79068524e-02, -1.12810783e-01, -6.66029975e-02,\n",
       "                            -2.84440517e-01],\n",
       "                           [-3.77485156e-01,  1.08872220e-01, -4.12505776e-01,\n",
       "                            -1.01115502e-01, -2.55110383e-01,  2.27715895e-01,\n",
       "                            -4.71140236e-01,  3.03983092e-01, -1.40530959e-01,\n",
       "                            -5.87317236e-02, -1.02145866e-01,  1.35886883e-02,\n",
       "                            -1.21349603e-01, -2.26397678e-01, -2.91189551e-01,\n",
       "                            -1.17084756e-01],\n",
       "                           [-2.27052178e-02,  2.92227387e-01,  5.78441396e-02,\n",
       "                             1.49758309e-01, -9.25782043e-03, -1.24476120e-01,\n",
       "                             2.83499658e-01, -3.12682748e-01, -1.37643889e-01,\n",
       "                            -3.22568089e-01, -4.55822647e-01, -2.31899679e-01,\n",
       "                            -4.48481739e-02, -3.21314633e-01,  1.07613288e-01,\n",
       "                             7.41016911e-03],\n",
       "                           [-9.48062986e-02,  7.14664087e-02, -2.05861032e-01,\n",
       "                             3.64036500e-01, -1.95076674e-01, -4.09929335e-01,\n",
       "                             6.08005047e-01,  1.56726569e-01,  1.76752910e-01,\n",
       "                             1.87650755e-01,  2.67248839e-01, -5.22821546e-02,\n",
       "                            -2.33103875e-02,  7.25705996e-02, -5.30078746e-02,\n",
       "                            -9.80744809e-02],\n",
       "                           [ 4.21521872e-01,  3.14847559e-01, -4.99759644e-01,\n",
       "                             3.91265512e-01,  2.16189176e-02, -1.35122880e-01,\n",
       "                            -5.19377328e-02, -1.99793592e-01, -4.22235802e-02,\n",
       "                             9.67920125e-02, -1.18316591e-01, -3.84340823e-01,\n",
       "                            -2.20712319e-01, -2.57784992e-01, -2.01250270e-01,\n",
       "                            -2.73090839e-01],\n",
       "                           [ 3.85212690e-01,  1.50727317e-01, -2.46275678e-01,\n",
       "                            -3.67831379e-01,  1.51598990e-01, -3.16909671e-01,\n",
       "                            -2.27637619e-01,  9.98283327e-02, -5.40916584e-02,\n",
       "                            -2.99106780e-02,  1.42330155e-01, -1.76829204e-01,\n",
       "                            -8.51180181e-02, -2.43302416e-02, -1.19699128e-01,\n",
       "                            -1.24271333e-01],\n",
       "                           [ 2.54896343e-01, -1.61466017e-01, -4.91541386e-01,\n",
       "                             9.34079364e-02, -2.79628217e-01,  1.24791242e-01,\n",
       "                            -6.15598336e-02, -4.77671057e-01,  7.41429180e-02,\n",
       "                             6.68165088e-03,  3.64494234e-01,  3.12541395e-01,\n",
       "                             2.43600868e-02, -2.91298833e-02,  8.58624652e-02,\n",
       "                            -2.70473093e-01],\n",
       "                           [ 2.30873302e-01, -9.18818489e-02, -4.19576705e-01,\n",
       "                             2.92079538e-01, -2.22083151e-01, -1.56634688e-01,\n",
       "                             1.88302174e-01,  7.98225626e-02,  2.82225609e-01,\n",
       "                             5.78326643e-01,  4.42917526e-01,  3.70087415e-01,\n",
       "                             8.38850290e-02,  2.29850680e-01,  4.46457118e-01,\n",
       "                             8.09310302e-02],\n",
       "                           [-5.44137321e-02, -4.97939764e-04,  3.10038894e-01,\n",
       "                             5.20392768e-02, -3.77024531e-01, -2.70874470e-01,\n",
       "                            -2.27165312e-01, -3.88886780e-02, -5.13816141e-02,\n",
       "                            -2.77596503e-01, -7.37346187e-02, -2.81812757e-01,\n",
       "                            -2.81732380e-01, -2.63150096e-01,  1.36090396e-02,\n",
       "                            -2.97765166e-01],\n",
       "                           [-2.21655488e-01, -2.83583403e-01, -4.90809321e-01,\n",
       "                             8.62029567e-02,  4.33869660e-02,  1.59468912e-02,\n",
       "                             1.47938192e-01,  9.84151775e-05,  4.57623780e-01,\n",
       "                             6.52554333e-02,  5.27997315e-01,  5.65818012e-01,\n",
       "                             5.45888066e-01,  1.29764661e-01,  6.16440117e-01,\n",
       "                             6.38927996e-01],\n",
       "                           [-2.90754884e-01,  3.87840629e-01, -2.51469582e-01,\n",
       "                            -1.91878095e-01, -2.62547601e-02, -1.81593508e-01,\n",
       "                             5.08061886e-01, -3.66040587e-01, -2.75792852e-02,\n",
       "                             1.98441565e-01, -2.74181906e-02, -8.31881538e-02,\n",
       "                             1.09768607e-01,  6.27502128e-02,  2.05529839e-01,\n",
       "                            -2.48558149e-02],\n",
       "                           [-3.21812809e-01,  2.06684709e-01,  1.69781044e-01,\n",
       "                            -3.56055737e-01,  2.94933110e-01, -3.05431604e-01,\n",
       "                            -4.53728259e-01,  2.75496036e-01, -4.41544324e-01,\n",
       "                             8.43053833e-02,  9.38978568e-02,  1.65228590e-01,\n",
       "                             5.72332703e-02,  2.15912014e-01, -4.61530715e-01,\n",
       "                            -2.98116922e-01],\n",
       "                           [-5.26211798e-01,  3.22813056e-02,  6.94765866e-01,\n",
       "                             1.46104246e-01, -1.90160155e-01, -4.21413854e-02,\n",
       "                             3.09110790e-01, -3.48784705e-03,  1.62786469e-01,\n",
       "                            -4.65561636e-02,  6.67557597e-01, -2.22942661e-02,\n",
       "                             2.99066633e-01,  1.27527803e-01,  1.02150373e-01,\n",
       "                             3.56361091e-01],\n",
       "                           [ 8.04439038e-02, -1.18196681e-01, -3.46841812e-02,\n",
       "                            -2.50468016e-01, -1.37951165e-01, -1.55010208e-01,\n",
       "                             6.80534542e-02,  2.51056105e-01,  5.23849666e-01,\n",
       "                             2.21668884e-01,  3.41312557e-01,  2.29113176e-01,\n",
       "                             2.45997861e-01,  4.34144586e-01,  3.94273460e-01,\n",
       "                             1.83854535e-01],\n",
       "                           [ 1.06877588e-01, -8.18378925e-02,  3.82060222e-02,\n",
       "                             1.54205307e-01,  4.22118932e-01,  6.36153594e-02,\n",
       "                             4.90116291e-02,  2.97722489e-01, -1.55809566e-01,\n",
       "                            -8.80980864e-02, -1.88334122e-01, -2.88602412e-01,\n",
       "                             1.34160250e-01, -1.95298910e-01,  2.07940102e-01,\n",
       "                            -4.48218361e-03],\n",
       "                           [-3.10350582e-02, -2.43413717e-01, -2.42339030e-01,\n",
       "                            -1.12358801e-01, -3.68945360e-01, -2.98845582e-02,\n",
       "                             1.37234986e-01,  8.35544020e-02, -3.11095834e-01,\n",
       "                            -3.78593564e-01, -2.52088994e-01, -4.58532244e-01,\n",
       "                            -1.87933430e-01, -1.36669397e-01, -3.18925798e-01,\n",
       "                            -3.49897444e-01],\n",
       "                           [-2.20737621e-01, -3.55776697e-01,  1.35463133e-01,\n",
       "                            -1.07671186e-01,  3.61715734e-01, -1.65730357e-01,\n",
       "                            -2.96805024e-01, -2.61861145e-01, -1.81379110e-01,\n",
       "                            -2.52679497e-01,  3.24638672e-02, -2.78514177e-01,\n",
       "                             3.91597897e-02, -3.04420084e-01,  1.31074801e-01,\n",
       "                            -4.58107382e-01],\n",
       "                           [ 2.57074416e-01, -7.89273828e-02, -1.74326569e-01,\n",
       "                            -2.85409719e-01, -1.93586480e-03,  8.19192752e-02,\n",
       "                             3.18837285e-01,  2.62260675e-01,  8.89771357e-02,\n",
       "                            -1.39398919e-02, -8.73171836e-02,  2.88430005e-02,\n",
       "                             1.95848048e-01, -1.06001601e-01,  1.31565899e-01,\n",
       "                             3.71359468e-01],\n",
       "                           [-2.54847229e-01, -5.04569411e-02, -2.26242486e-02,\n",
       "                             5.93276210e-02,  8.76985211e-03,  5.32612860e-01,\n",
       "                             8.35426748e-02, -3.84508133e-01,  2.76893616e-01,\n",
       "                             3.82656194e-02, -7.21498579e-02, -1.81282207e-01,\n",
       "                             7.34369382e-02,  1.17235899e-01, -6.51881456e-01,\n",
       "                            -3.34500521e-01],\n",
       "                           [-1.70137778e-01, -1.21073164e-01,  1.78313926e-01,\n",
       "                            -2.78807014e-01,  1.08801238e-01, -1.11947298e-01,\n",
       "                            -3.89023125e-01, -1.61864370e-01,  3.43064666e-01,\n",
       "                             2.71567851e-02,  4.07976389e-01,  4.18348342e-01,\n",
       "                             4.64583427e-01,  3.53946656e-01,  3.89542520e-01,\n",
       "                             1.67698279e-01],\n",
       "                           [ 4.31533337e-01,  3.21526825e-01,  3.23250115e-01,\n",
       "                             1.91775545e-01,  3.02342534e-01,  1.15902133e-01,\n",
       "                            -2.59978950e-01,  9.85879302e-02, -7.18907341e-02,\n",
       "                            -9.21317376e-03, -5.44300750e-02, -8.10516924e-02,\n",
       "                            -9.18582901e-02,  5.40590063e-02,  4.60461564e-02,\n",
       "                            -8.35334510e-02],\n",
       "                           [-9.98775065e-02,  1.75724879e-01,  2.56683916e-01,\n",
       "                            -3.59743565e-01, -2.93287814e-01,  1.62838116e-01,\n",
       "                            -1.37296140e-01,  1.37855083e-01, -6.09246604e-02,\n",
       "                             1.99194327e-01, -7.22568408e-02, -3.34399760e-01,\n",
       "                            -7.06986859e-02, -2.31320500e-01, -2.18023434e-01,\n",
       "                            -4.87150341e-01],\n",
       "                           [-3.68396193e-01,  2.67442882e-01, -3.94970387e-01,\n",
       "                             1.77428842e-01,  3.26791018e-01, -2.87294865e-01,\n",
       "                             3.02136213e-01,  2.36927867e-01, -3.55912179e-01,\n",
       "                            -3.33914578e-01,  8.07485264e-03,  4.70914431e-02,\n",
       "                            -3.69885653e-01, -1.27482638e-01, -3.96117955e-01,\n",
       "                            -5.72469793e-02],\n",
       "                           [-1.98528856e-01,  5.31884611e-01, -2.34133899e-01,\n",
       "                             1.98080614e-01,  2.62376428e-01, -3.72010134e-02,\n",
       "                            -9.44984034e-02, -2.12096497e-02,  1.58407092e-01,\n",
       "                             2.83923328e-01,  1.14997894e-01,  1.66923046e-01,\n",
       "                            -3.98381203e-02,  1.89566299e-01,  1.66740984e-01,\n",
       "                             2.68138558e-01],\n",
       "                           [-3.16627711e-01, -2.85716951e-01,  2.55819373e-02,\n",
       "                             3.49407434e-01, -4.93821204e-02, -1.23138288e-02,\n",
       "                            -6.87777400e-02, -1.73625588e-01,  4.82089221e-01,\n",
       "                             5.95344067e-01,  3.55598420e-01,  2.68973768e-01,\n",
       "                             2.93905456e-02, -1.42440498e-01,  6.54410899e-01,\n",
       "                             2.06725121e-01],\n",
       "                           [ 2.88134098e-01,  2.26236403e-01,  2.28432566e-03,\n",
       "                            -1.08551860e-01, -1.19869567e-01,  8.62033591e-02,\n",
       "                             3.56496364e-01, -1.35885701e-02,  2.14536980e-01,\n",
       "                             3.98211032e-01,  3.14282090e-03,  5.86493433e-01,\n",
       "                             4.14420873e-01,  5.46300650e-01,  2.51665652e-01,\n",
       "                             3.73932868e-01],\n",
       "                           [ 1.61496792e-02,  1.11286677e-01, -1.75296143e-02,\n",
       "                             1.13855794e-01, -2.65161157e-01,  3.59887958e-01,\n",
       "                            -2.13755205e-01, -2.85101026e-01,  6.03097752e-02,\n",
       "                             1.30821869e-01, -2.11902902e-01, -1.00064985e-01,\n",
       "                             1.18587539e-01, -1.52777925e-01, -2.20421702e-01,\n",
       "                            -3.56815122e-02],\n",
       "                           [ 6.61568791e-02, -3.12794924e-01, -1.90016463e-01,\n",
       "                            -3.95417392e-01,  1.46319509e-01,  2.01224148e-01,\n",
       "                             3.40472497e-02, -1.27573505e-01,  1.45517901e-01,\n",
       "                             6.55191615e-02, -3.56628627e-01,  9.94208306e-02,\n",
       "                            -1.13631360e-01, -2.19984949e-02, -3.25719416e-01,\n",
       "                            -1.30170405e-01],\n",
       "                           [ 3.90349030e-01,  1.70273170e-01, -3.03755969e-01,\n",
       "                             2.15014905e-01, -4.14140895e-02, -9.65475515e-02,\n",
       "                            -3.09017263e-02, -2.66810149e-01,  2.08412290e-01,\n",
       "                             3.17697704e-01,  4.58192706e-01,  1.77695289e-01,\n",
       "                             3.19002926e-01,  3.01114231e-01,  1.07879847e-01,\n",
       "                             3.28563243e-01]], dtype=float32),\n",
       "          },\n",
       "      },\n",
       "  })),\n",
       " {'eval/episode_reward': DeviceArray(7446.799, dtype=float32),\n",
       "  'eval/episode_reward_contact_cost': DeviceArray(0.72900707, dtype=float32),\n",
       "  'eval/episode_reward_ctrl_cost': DeviceArray(772.87787, dtype=float32),\n",
       "  'eval/episode_reward_forward': DeviceArray(7242.8594, dtype=float32),\n",
       "  'eval/episode_reward_survive': DeviceArray(977.5469, dtype=float32),\n",
       "  'losses/entropy_loss': DeviceArray(0.04928277, dtype=float32),\n",
       "  'losses/policy_loss': DeviceArray(-0.00038278, dtype=float32),\n",
       "  'losses/total_loss': DeviceArray(0.08547745, dtype=float32),\n",
       "  'losses/v_loss': DeviceArray(0.03657747, dtype=float32),\n",
       "  'eval/completed_episodes': DeviceArray(128., dtype=float32),\n",
       "  'eval/avg_episode_length': DeviceArray(977.5469, dtype=float32),\n",
       "  'speed/sps': DeviceArray(411170.53, dtype=float32),\n",
       "  'speed/eval_sps': 55512.491814591674,\n",
       "  'speed/training_walltime': 78.92587876319885,\n",
       "  'speed/eval_walltime': 30.744287252426147,\n",
       "  'speed/timestamp': 78.92587876319885})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brax.envs import create_fn\n",
    "from brax.training import ppo\n",
    "from classroom.brax import BraxRecorder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def env_fn(name: str, db_path: str):\n",
    "    brax_fn = create_fn(env_name=name)\n",
    "    return lambda *args, **kwargs: BraxRecorder(brax_fn(*args, **kwargs), db_path)\n",
    "\n",
    "\n",
    "inference, params, metrics = ppo.train(\n",
    "    environment_fn=env_fn('ant', '~/classroom/ant'), num_timesteps = 30_000_000,\n",
    "    log_frequency = 10, reward_scaling = .1, episode_length = 1000,\n",
    "    normalize_observations = True, action_repeat = 1, unroll_length = 5,\n",
    "    num_minibatches = 32, num_update_epochs = 4, discounting = 0.97,\n",
    "    learning_rate = 3e-4, entropy_cost = 1e-2, num_envs = 2048,\n",
    "    batch_size = 1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like, you can now use the Classroom GUI to input your own preferences on the resulting clips. For this example, though, we'll simulate the human preference feedback by stochastically generating preferences based on the ground truth rewards. `SyntheticPairwisePrefs` samples the preference for each pair of clips from a mixture distribution with two components:\n",
    "- a distribution based on the [Bradley-Terry model](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model) of pairwise comparison, where the probability of preferring A to B is equal to the sigmoid of the difference in \"scores\" of A and B, and\n",
    "- a uniform distribution over the two clips, representing the possibility of the human making a \"mistake\" and selecting a clip at random.\n",
    "\n",
    "The \"scores\" for the Bradley-Terry distribution are the rescaled sums of rewards for each clip. `SyntheticPairwisePrefs` normalizes the rewards to zero mean and unit variance, then scales them by a \"rationality\" parameter β which defaults to 5.0. You can also choose the weight assigned to the mistake distribution with the `mistake_prob` parameter; by default it is set to 0.1.\n",
    "\n",
    "Note that the total number of pairwise comparisons in any dataset scales quadratically with the dataset size, which could easily get unwieldy with even medium-size datasets. Training a model on all `N(N - 1) // 2` pairs of clips could also lead to overfitting since the model will see each clip many times per epoch. To prevent this, `SyntheticPairwisePrefs` samples a subset of `N` clip pairs _without replacement_ by shuffling the dataset and creating pairs from adjacent clips in the shuffled dataset. The random seed used to sample clip pairs and preferences can be set using the `seed` parameter and defaults to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading rewards from /home/nora/value-learning/ant/seed_0/rewards.csv\n",
      "Loading rewards from /home/nora/value-learning/ant/seed_1/rewards.csv\n",
      "Loading rewards from /home/nora/value-learning/ant/seed_2/rewards.csv\n",
      "Loading rewards from /home/nora/value-learning/ant/seed_3/rewards.csv\n",
      "Loading rewards from /home/nora/value-learning/ant/seed_4/rewards.csv\n",
      "Loading rewards from /home/nora/value-learning/ant/seed_5/rewards.csv\n",
      "Loading rewards from /home/nora/value-learning/ant/seed_6/rewards.csv\n",
      "Loading rewards from /home/nora/value-learning/ant/seed_7/rewards.csv\n",
      "Loading rewards from /home/nora/value-learning/ant/seed_8/rewards.csv\n",
      "Loading rewards from /home/nora/value-learning/ant/seed_9/rewards.csv\n"
     ]
    }
   ],
   "source": [
    "from classroom.brax import flatten_clip\n",
    "from classroom.datasets import SyntheticPairwisePrefs, EnsembleDataset\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ds = SyntheticPairwisePrefs(\n",
    "    '/home/nora/value-learning/ant',\n",
    "    in_memory=True,\n",
    "    transform=flatten_clip\n",
    ")\n",
    "ds = EnsembleDataset(ds, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classroom.datasets import BatchedDataset, SubsetDataset\n",
    "import numpy as np\n",
    "\n",
    "indices = np.random.permutation(len(ds))\n",
    "test_size = round(len(ds) * 0.2)\n",
    "\n",
    "test = SubsetDataset(ds, indices[:test_size])\n",
    "# val = SubsetDataset(ds, indices[test_size:test_size * 2])\n",
    "# train = SubsetDataset(ds, indices[test_size * 2:])\n",
    "val = BatchedDataset(SubsetDataset(ds, indices[test_size:test_size * 2]), 64, dim=1)\n",
    "train = BatchedDataset(SubsetDataset(ds, indices[test_size * 2:]), 64, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend implementing your preference model as a subclass of `classroom.jax.PairwisePrefModel` if possible, since this base class implements most of the training loop boilerplate code for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classroom.jax import ListwisePrefModel, PairwisePrefModel\n",
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "class MlpPrefModel(PairwisePrefModel):\n",
    "    hidden_layers: list[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        for size in self.hidden_layers:\n",
    "            x = nn.Dense(size)(x)\n",
    "            x = nn.activation.relu(x)\n",
    "            # x = nn.Dropout(0.1)(x, deterministic=eval)\n",
    "        \n",
    "        # scores, var = RandomFeatureGaussianProcess(1)(x)\n",
    "        scores = nn.Dense(1)(x)\n",
    "        return scores.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model is as simple as calling `model.fit()` with your training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.450, val acc: 0.866 (sd 0.001), val loss: 0.371 (sd 0.002)\n",
      "Epoch 2 train loss: 0.367, val acc: 0.868 (sd 0.001), val loss: 0.367 (sd 0.001)\n",
      "Epoch 3 train loss: 0.364, val acc: 0.869 (sd 0.000), val loss: 0.365 (sd 0.001)\n",
      "Epoch 4 train loss: 0.363, val acc: 0.869 (sd 0.001), val loss: 0.364 (sd 0.001)\n",
      "Epoch 5 train loss: 0.362, val acc: 0.869 (sd 0.001), val loss: 0.364 (sd 0.001)\n",
      "Epoch 6 train loss: 0.361, val acc: 0.869 (sd 0.001), val loss: 0.363 (sd 0.001)\n",
      "Epoch 7 train loss: 0.361, val acc: 0.869 (sd 0.001), val loss: 0.363 (sd 0.001)\n",
      "Epoch 8 train loss: 0.360, val acc: 0.869 (sd 0.001), val loss: 0.363 (sd 0.001)\n",
      "Epoch 9 train loss: 0.360, val acc: 0.869 (sd 0.001), val loss: 0.363 (sd 0.001)\n",
      "Epoch 10 train loss: 0.359, val acc: 0.869 (sd 0.001), val loss: 0.362 (sd 0.001)\n",
      "Epoch 11 train loss: 0.359, val acc: 0.869 (sd 0.001), val loss: 0.362 (sd 0.001)\n",
      "Epoch 12 train loss: 0.359, val acc: 0.869 (sd 0.001), val loss: 0.362 (sd 0.001)\n",
      "Epoch 13 train loss: 0.358, val acc: 0.869 (sd 0.001), val loss: 0.362 (sd 0.001)\n",
      "Epoch 14 train loss: 0.358, val acc: 0.869 (sd 0.001), val loss: 0.362 (sd 0.001)\n",
      "Epoch 15 train loss: 0.358, val acc: 0.869 (sd 0.001), val loss: 0.362 (sd 0.001)\n",
      "Epoch 16 train loss: 0.357, val acc: 0.869 (sd 0.001), val loss: 0.362 (sd 0.001)\n",
      "Epoch 17 train loss: 0.357, val acc: 0.869 (sd 0.001), val loss: 0.362 (sd 0.001)\n",
      "Epoch 18 train loss: 0.357, val acc: 0.869 (sd 0.001), val loss: 0.362 (sd 0.001)\n",
      "Epoch 19 train loss: 0.356, val acc: 0.868 (sd 0.001), val loss: 0.361 (sd 0.001)\n",
      "Epoch 20 train loss: 0.356, val acc: 0.868 (sd 0.001), val loss: 0.361 (sd 0.001)\n",
      "Epoch 21 train loss: 0.356, val acc: 0.869 (sd 0.000), val loss: 0.361 (sd 0.001)\n",
      "Epoch 22 train loss: 0.356, val acc: 0.868 (sd 0.001), val loss: 0.361 (sd 0.001)\n",
      "Epoch 23 train loss: 0.355, val acc: 0.868 (sd 0.000), val loss: 0.361 (sd 0.001)\n",
      "Epoch 24 train loss: 0.355, val acc: 0.868 (sd 0.000), val loss: 0.361 (sd 0.001)\n",
      "Epoch 25 train loss: 0.355, val acc: 0.868 (sd 0.000), val loss: 0.361 (sd 0.001)\n",
      "Epoch 26 train loss: 0.354, val acc: 0.868 (sd 0.000), val loss: 0.361 (sd 0.002)\n",
      "Epoch 27 train loss: 0.354, val acc: 0.868 (sd 0.000), val loss: 0.361 (sd 0.002)\n",
      "Epoch 28 train loss: 0.354, val acc: 0.868 (sd 0.001), val loss: 0.361 (sd 0.002)\n",
      "Epoch 29 train loss: 0.354, val acc: 0.867 (sd 0.001), val loss: 0.361 (sd 0.002)\n",
      "Epoch 30 train loss: 0.353, val acc: 0.867 (sd 0.001), val loss: 0.361 (sd 0.002)\n",
      "Epoch 31 train loss: 0.353, val acc: 0.867 (sd 0.001), val loss: 0.361 (sd 0.002)\n",
      "Epoch 32 train loss: 0.353, val acc: 0.867 (sd 0.001), val loss: 0.361 (sd 0.002)\n",
      "Epoch 33 train loss: 0.352, val acc: 0.867 (sd 0.001), val loss: 0.361 (sd 0.002)\n",
      "Epoch 34 train loss: 0.352, val acc: 0.867 (sd 0.001), val loss: 0.361 (sd 0.002)\n",
      "Epoch 35 train loss: 0.352, val acc: 0.867 (sd 0.001), val loss: 0.360 (sd 0.002)\n",
      "Epoch 36 train loss: 0.351, val acc: 0.867 (sd 0.001), val loss: 0.360 (sd 0.002)\n",
      "Epoch 37 train loss: 0.351, val acc: 0.867 (sd 0.001), val loss: 0.360 (sd 0.002)\n",
      "Epoch 38 train loss: 0.351, val acc: 0.867 (sd 0.002), val loss: 0.360 (sd 0.002)\n",
      "Epoch 39 train loss: 0.351, val acc: 0.867 (sd 0.002), val loss: 0.360 (sd 0.002)\n",
      "Epoch 40 train loss: 0.350, val acc: 0.867 (sd 0.002), val loss: 0.360 (sd 0.002)\n",
      "Epoch 41 train loss: 0.350, val acc: 0.866 (sd 0.002), val loss: 0.360 (sd 0.002)\n",
      "Epoch 42 train loss: 0.350, val acc: 0.867 (sd 0.002), val loss: 0.360 (sd 0.002)\n",
      "Epoch 43 train loss: 0.349, val acc: 0.866 (sd 0.001), val loss: 0.360 (sd 0.002)\n",
      "Epoch 44 train loss: 0.349, val acc: 0.866 (sd 0.002), val loss: 0.360 (sd 0.002)\n",
      "Epoch 45 train loss: 0.349, val acc: 0.866 (sd 0.002), val loss: 0.360 (sd 0.002)\n",
      "Epoch 46 train loss: 0.348, val acc: 0.866 (sd 0.001), val loss: 0.360 (sd 0.002)\n",
      "Epoch 47 train loss: 0.348, val acc: 0.866 (sd 0.002), val loss: 0.360 (sd 0.002)\n",
      "Epoch 48 train loss: 0.348, val acc: 0.866 (sd 0.002), val loss: 0.360 (sd 0.002)\n",
      "Epoch 49 train loss: 0.347, val acc: 0.866 (sd 0.002), val loss: 0.360 (sd 0.002)\n",
      "Epoch 50 train loss: 0.347, val acc: 0.866 (sd 0.002), val loss: 0.360 (sd 0.002)\n",
      "Epoch 51 train loss: 0.347, val acc: 0.866 (sd 0.002), val loss: 0.360 (sd 0.002)\n",
      "Epoch 52 train loss: 0.346, val acc: 0.866 (sd 0.002), val loss: 0.360 (sd 0.002)\n",
      "Epoch 53 train loss: 0.346, val acc: 0.866 (sd 0.002), val loss: 0.360 (sd 0.002)\n",
      "Epoch 54 train loss: 0.346, val acc: 0.866 (sd 0.002), val loss: 0.360 (sd 0.002)\n",
      "Epoch 55 train loss: 0.345, val acc: 0.866 (sd 0.002), val loss: 0.360 (sd 0.002)\n",
      "Epoch 56 train loss: 0.345, val acc: 0.866 (sd 0.002), val loss: 0.360 (sd 0.002)\n",
      "Epoch 57 train loss: 0.345, val acc: 0.866 (sd 0.002), val loss: 0.360 (sd 0.002)\n",
      "Epoch 58 train loss: 0.344, val acc: 0.866 (sd 0.002), val loss: 0.360 (sd 0.003)\n",
      "Epoch 59 train loss: 0.344, val acc: 0.866 (sd 0.002), val loss: 0.360 (sd 0.003)\n",
      "Epoch 60 train loss: 0.344, val acc: 0.866 (sd 0.003), val loss: 0.360 (sd 0.003)\n",
      "Epoch 61 train loss: 0.343, val acc: 0.866 (sd 0.002), val loss: 0.360 (sd 0.003)\n",
      "Epoch 62 train loss: 0.343, val acc: 0.866 (sd 0.003), val loss: 0.359 (sd 0.003)\n",
      "Epoch 63 train loss: 0.343, val acc: 0.866 (sd 0.002), val loss: 0.359 (sd 0.003)\n",
      "Epoch 64 train loss: 0.342, val acc: 0.866 (sd 0.002), val loss: 0.359 (sd 0.003)\n",
      "Epoch 65 train loss: 0.342, val acc: 0.866 (sd 0.002), val loss: 0.359 (sd 0.003)\n",
      "Epoch 66 train loss: 0.342, val acc: 0.865 (sd 0.002), val loss: 0.359 (sd 0.003)\n",
      "Epoch 67 train loss: 0.341, val acc: 0.865 (sd 0.002), val loss: 0.359 (sd 0.003)\n",
      "Early stopping; val loss plateaued at 0.359\n"
     ]
    }
   ],
   "source": [
    "model = MlpPrefModel([32, 16])\n",
    "state = model.fit(train, val, seed=jnp.arange(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if we can train an agent using the preference model we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brax.envs.env import Env, State, Wrapper\n",
    "from classroom.jax import PrefModel\n",
    "from dataclasses import dataclass\n",
    "from jax.experimental.host_callback import id_tap\n",
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "env_rewards = []\n",
    "model_rewards = []\n",
    "\n",
    "@dataclass\n",
    "class PrefEnsemble:\n",
    "    model: PrefModel\n",
    "    variables: dict\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self._predict_fn = jax.jit(\n",
    "            jax.vmap(\n",
    "                lambda variables, x: self.model.apply(variables, x),\n",
    "                in_axes=(0, None)   # type: ignore\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        return self._predict_fn(self.variables, x)\n",
    "\n",
    "class BraxEvaluator(Wrapper):\n",
    "    def __init__(self, env: Env, ensemble: PrefEnsemble):\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.env = env\n",
    "        # self.env_rewards = []\n",
    "        self.ensemble = ensemble\n",
    "    \n",
    "    def step(self, state: State, action: jnp.ndarray) -> State:\n",
    "        next_state = super().step(state, action)\n",
    "        env_reward = next_state.reward.mean()\n",
    "        id_tap(\n",
    "            lambda rew, _: env_rewards.append(rew),\n",
    "            env_reward\n",
    "        )\n",
    "        # return next_state\n",
    "\n",
    "        transition = jnp.concatenate([state.obs, action], axis=-1)\n",
    "        scores = self.ensemble(transition).min(axis=0).clip(-1)\n",
    "\n",
    "        id_tap(\n",
    "            lambda rew, _: model_rewards.append(rew),\n",
    "            scores.mean()\n",
    "        )\n",
    "        return next_state.replace(reward=scores)  # type: ignore[attr-defined]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3dd8061330>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(\n",
    "    np.arange(len(env_rewards)),\n",
    "    env_rewards\n",
    ")\n",
    "# plt.plot(\n",
    "#     np.arange(len(model_rewards)),\n",
    "#     model_rewards,\n",
    "#     c='red'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nora/miniconda3/lib/python3.10/site-packages/jax/_src/tree_util.py:188: FutureWarning: jax.tree_util.tree_multimap() is deprecated. Please use jax.tree_util.tree_map() instead as a drop-in replacement.\n",
      "  warnings.warn('jax.tree_util.tree_multimap() is deprecated. Please use jax.tree_util.tree_map() '\n"
     ]
    }
   ],
   "source": [
    "from brax.envs import create_fn\n",
    "from brax.training import ppo\n",
    "import numpy as np\n",
    "\n",
    "metric_history = []\n",
    "\n",
    "def progress(_, metrics):\n",
    "    metric_history.append(metrics)\n",
    "\n",
    "def env_fn(name: str, ensemble: PrefEnsemble):\n",
    "    brax_fn = create_fn(env_name=name)\n",
    "    return lambda *args, **kwargs: BraxEvaluator(brax_fn(*args, **kwargs), ensemble)\n",
    "\n",
    "\n",
    "ensemble = PrefEnsemble(model, {'params': state.params})\n",
    "inference, params, metrics = ppo.train(\n",
    "    environment_fn=env_fn('ant', ensemble),\n",
    "    num_timesteps = 30_000_000,\n",
    "    log_frequency = 10, reward_scaling = .1, episode_length = 1000,\n",
    "    normalize_observations = True, action_repeat = 1, unroll_length = 5,\n",
    "    num_minibatches = 32, num_update_epochs = 4, discounting = 0.97,\n",
    "    learning_rate = 3e-4, entropy_cost = 1e-2, num_envs = 2048,\n",
    "    batch_size = 1024, progress_fn = progress\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa81541598839f570aad5dc7c0c2a1b79738f10fbd2c927ab4c8b970fa51d8f7"
  },
  "kernelspec": {
   "display_name": "Conda (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
